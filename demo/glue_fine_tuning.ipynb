{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glue_fine-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaa2BdMZz9Ua"
      },
      "source": [
        "# Fine-tuning Transformer models for GLUE tasks, using *torchdistill*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cTkGsg0I6K"
      },
      "source": [
        "## 1. Make sure you have access to GPU/TPU\n",
        "Google Colab: Runtime -> Change runtime type -> Hardware accelarator: \"GPU\" or \"TPU\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7U6G5N8z06c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd5b8a8-3473-4484-eabb-e80a2316d5cc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  7 19:54:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtaTzdTy0mMg"
      },
      "source": [
        "## 2. Clone torchdistill repository to use its example code and configuration files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR5GGkREVl3s",
        "outputId": "ec45f3e8-95cc-44b8-c6ea-ce8a9eb4382b"
      },
      "source": [
        "!git clone https://github.com/yoshitomo-matsubara/torchdistill"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'torchdistill'...\n",
            "remote: Enumerating objects: 4784, done.\u001b[K\n",
            "remote: Counting objects: 100% (566/566), done.\u001b[K\n",
            "remote: Compressing objects: 100% (352/352), done.\u001b[K\n",
            "remote: Total 4784 (delta 325), reused 386 (delta 188), pack-reused 4218\u001b[K\n",
            "Receiving objects: 100% (4784/4784), 1.02 MiB | 10.84 MiB/s, done.\n",
            "Resolving deltas: 100% (2939/2939), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgzJAnV00UN8"
      },
      "source": [
        "## 3. Install dependencies and *torchdistill*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz9Y_IpzevFw",
        "outputId": "b462cd5c-930b-4ca0-a5a0-d5cb760989ef"
      },
      "source": [
        "!pip install -r torchdistill/examples/hf_transformers/requirements.txt\n",
        "!pip install torchdistill"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/fa/d173d923c953d930702066894abf128a7e5258c6f64cf088d2c5a83f46a3/accelerate-0.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25hCollecting datasets>=1.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 9.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r torchdistill/examples/hf_transformers/requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from -r torchdistill/examples/hf_transformers/requirements.txt (line 5)) (1.8.1+cu101)\n",
            "Collecting transformers>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 31.7MB/s \n",
            "\u001b[?25hCollecting pyaml>=20.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (1.19.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 58.1MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (20.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (3.10.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r torchdistill/examples/hf_transformers/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->-r torchdistill/examples/hf_transformers/requirements.txt (line 4)) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->-r torchdistill/examples/hf_transformers/requirements.txt (line 5)) (3.7.4.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.5.1->-r torchdistill/examples/hf_transformers/requirements.txt (line 6)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.5.1->-r torchdistill/examples/hf_transformers/requirements.txt (line 6)) (3.0.12)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate->-r torchdistill/examples/hf_transformers/requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r torchdistill/examples/hf_transformers/requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.5.1->-r torchdistill/examples/hf_transformers/requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.5.1->-r torchdistill/examples/hf_transformers/requirements.txt (line 6)) (7.1.2)\n",
            "Installing collected packages: pyaml, accelerate, xxhash, huggingface-hub, fsspec, datasets, sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed accelerate-0.3.0 datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 pyaml-20.4.0 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1 xxhash-2.0.2\n",
            "Collecting torchdistill\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/1e/98c4591040d5ba7b849432e4bc6a575a8c87aa228fa043cbfb1ead9695be/torchdistill-0.2.0-py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchdistill) (1.19.5)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from torchdistill) (0.9.1+cu101)\n",
            "Collecting pyyaml>=5.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from torchdistill) (1.8.1+cu101)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from torchdistill) (0.29.22)\n",
            "Requirement already satisfied: pycocotools>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from torchdistill) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torchdistill) (1.4.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.2->torchdistill) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.1->torchdistill) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.1->torchdistill) (56.1.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.1->torchdistill) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.1->torchdistill) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.1->torchdistill) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.1->torchdistill) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.1->torchdistill) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools>=2.0.1->torchdistill) (1.15.0)\n",
            "Installing collected packages: pyyaml, torchdistill\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.4.1 torchdistill-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgvtpJmHSGXr"
      },
      "source": [
        "## (Optional) Configure Accelerate for 2x-speedup training by mixed-precision\n",
        "\n",
        "If you are **NOT** using the Google Colab Pro, it will exceed 12 hours (maximum lifetimes for free Google Colab users) to fine-tune a base-sized model for the following 9 different tasks with Tesla K80.\n",
        "By using mixed-precision training, you can complete all the 9 fine-tuning jobs.\n",
        "[This table](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification#mixed-precision-training) gives you a good idea about how long it will take to fine-tune a BERT-Base on a Titan RTX with/without mixed-precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGI5L9W6SEfT",
        "outputId": "155c3b2f-ffcc-41f7-9044-5d3c0dcc564a"
      },
      "source": [
        "!accelerate config"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-GPU, [2] TPU): 0\n",
            "How many processes in total will you use? [1]: 1\n",
            "Do you wish to use FP16 (mixed precision)? [yes/NO]: yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxppgvR51Ij1"
      },
      "source": [
        "## 4. Fine-tuning Transformer models for GLUE tasks\n",
        "The following examples demonstrate how to fine-tune pretrained BERT-Base (uncased) on each of datasets in GLUE.  \n",
        "**Note**: Test splits for GLUE tasks in `datasets` package are not labeled, and you use only training and validation spltis in this example, following [Hugging Face's example](https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFHCWbIG1paE"
      },
      "source": [
        "### 4.1 CoLA task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oFHTV4jV7yN",
        "outputId": "4c37ceaf-f6ff-423a-ef2b-e35fd71ac60f"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/cola/ce/bert_base_uncased.yaml \\\n",
        "  --task cola \\\n",
        "  --log log/glue/cola/ce/bert_base_uncased.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 19:55:19.941256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/07 19:55:20\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/cola/ce/bert_base_uncased.yaml', log='log/glue/cola/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='cola', test_only=False, world_size=1)\n",
            "2021/05/07 19:55:21\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "2021/05/07 19:55:21\tINFO\tfilelock\tLock 140175863705040 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph8kh83h5\n",
            "Downloading: 100% 570/570 [00:00<00:00, 556kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "2021/05/07 19:55:21\tINFO\tfilelock\tLock 140175863705040 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2021/05/07 19:55:21\tINFO\tfilelock\tLock 140175871360528 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzgl20s7r\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 919kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "2021/05/07 19:55:22\tINFO\tfilelock\tLock 140175871360528 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "2021/05/07 19:55:22\tINFO\tfilelock\tLock 140175871360976 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkljitc1w\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.42MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "2021/05/07 19:55:23\tINFO\tfilelock\tLock 140175871360976 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
            "2021/05/07 19:55:23\tINFO\tfilelock\tLock 140175864096144 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdd_mud5i\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 26.7kB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "2021/05/07 19:55:23\tINFO\tfilelock\tLock 140175864096144 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "2021/05/07 19:55:24\tINFO\tfilelock\tLock 140175864096144 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
            "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcmjq0dst\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 66.1MB/s]\n",
            "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "2021/05/07 19:55:31\tINFO\tfilelock\tLock 140175864096144 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 28.8kB [00:00, 21.1MB/s]       \n",
            "Downloading: 28.7kB [00:00, 26.4MB/s]       \n",
            "Downloading and preparing dataset glue/cola (download: 368.14 KiB, generated: 596.73 KiB, post-processed: Unknown size, total: 964.86 KiB) to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 377k/377k [00:00<00:00, 3.49MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 9/9 [00:00<00:00, 21.30ba/s]\n",
            "100% 2/2 [00:00<00:00, 55.12ba/s]\n",
            "100% 2/2 [00:00<00:00, 56.52ba/s]\n",
            "Downloading: 5.75kB [00:00, 5.08MB/s]       \n",
            "2021/05/07 19:55:37\tINFO\t__main__\tStart training\n",
            "2021/05/07 19:55:37\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/07 19:55:37\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/07 19:55:37\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/07 19:55:48\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [  0/268]  eta: 0:01:05  lr: 1.9975124378109453e-05  sample/s: 17.729990214126744  loss: 0.5688 (0.5688)  time: 0.2459  data: 0.0202  max mem: 1854\n",
            "2021/05/07 19:55:53\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 50/268]  eta: 0:00:23  lr: 1.873134328358209e-05  sample/s: 42.050162789706775  loss: 0.5678 (0.5948)  time: 0.1007  data: 0.0030  max mem: 2534\n",
            "2021/05/07 19:55:58\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [100/268]  eta: 0:00:17  lr: 1.7487562189054726e-05  sample/s: 37.71279570569248  loss: 0.5542 (0.5766)  time: 0.0988  data: 0.0031  max mem: 2534\n",
            "2021/05/07 19:56:04\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [150/268]  eta: 0:00:12  lr: 1.6243781094527366e-05  sample/s: 42.06186476796952  loss: 0.5001 (0.5511)  time: 0.1030  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:56:09\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [200/268]  eta: 0:00:06  lr: 1.5000000000000002e-05  sample/s: 37.6201128342198  loss: 0.4710 (0.5290)  time: 0.1028  data: 0.0031  max mem: 2679\n",
            "2021/05/07 19:56:14\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [250/268]  eta: 0:00:01  lr: 1.3756218905472638e-05  sample/s: 41.63091223551423  loss: 0.4697 (0.5168)  time: 0.1003  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:56:15\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:27\n",
            "2021/05/07 19:56:16\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "2021/05/07 19:56:16\tINFO\t__main__\tValidation: matthews_correlation = 0.5060974277315389\n",
            "2021/05/07 19:56:16\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 19:56:18\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [  0/268]  eta: 0:00:28  lr: 1.3308457711442788e-05  sample/s: 39.500432034091844  loss: 0.4642 (0.4642)  time: 0.1058  data: 0.0045  max mem: 2679\n",
            "2021/05/07 19:56:23\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 50/268]  eta: 0:00:22  lr: 1.2064676616915423e-05  sample/s: 37.57091286938918  loss: 0.4054 (0.4040)  time: 0.1009  data: 0.0032  max mem: 2679\n",
            "2021/05/07 19:56:28\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [100/268]  eta: 0:00:17  lr: 1.082089552238806e-05  sample/s: 40.7629525244181  loss: 0.3905 (0.3923)  time: 0.1026  data: 0.0032  max mem: 2679\n",
            "2021/05/07 19:56:33\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [150/268]  eta: 0:00:12  lr: 9.577114427860697e-06  sample/s: 41.98880780050255  loss: 0.3717 (0.3836)  time: 0.1042  data: 0.0031  max mem: 2679\n",
            "2021/05/07 19:56:38\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [200/268]  eta: 0:00:06  lr: 8.333333333333334e-06  sample/s: 41.67537987276715  loss: 0.3886 (0.3884)  time: 0.1019  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:56:43\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [250/268]  eta: 0:00:01  lr: 7.089552238805971e-06  sample/s: 37.34319820331159  loss: 0.3442 (0.3793)  time: 0.1048  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:56:45\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:27\n",
            "2021/05/07 19:56:45\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "2021/05/07 19:56:45\tINFO\t__main__\tValidation: matthews_correlation = 0.537773817191238\n",
            "2021/05/07 19:56:45\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 19:56:47\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [  0/268]  eta: 0:00:27  lr: 6.64179104477612e-06  sample/s: 40.34196813946498  loss: 0.3842 (0.3842)  time: 0.1036  data: 0.0044  max mem: 2679\n",
            "2021/05/07 19:56:52\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 50/268]  eta: 0:00:22  lr: 5.398009950248757e-06  sample/s: 41.92103145848429  loss: 0.2355 (0.2677)  time: 0.1031  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:56:57\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [100/268]  eta: 0:00:17  lr: 4.1542288557213935e-06  sample/s: 41.612738885099525  loss: 0.2661 (0.2678)  time: 0.1036  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:57:02\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [150/268]  eta: 0:00:12  lr: 2.9104477611940303e-06  sample/s: 41.59612829006089  loss: 0.2948 (0.2788)  time: 0.1021  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:57:08\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [200/268]  eta: 0:00:06  lr: 1.6666666666666667e-06  sample/s: 41.96328728611555  loss: 0.2074 (0.2735)  time: 0.1000  data: 0.0030  max mem: 2679\n",
            "2021/05/07 19:57:13\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [250/268]  eta: 0:00:01  lr: 4.2288557213930354e-07  sample/s: 37.21282480675177  loss: 0.2429 (0.2737)  time: 0.1029  data: 0.0031  max mem: 2679\n",
            "2021/05/07 19:57:14\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:27\n",
            "2021/05/07 19:57:15\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "2021/05/07 19:57:15\tINFO\t__main__\tValidation: matthews_correlation = 0.546773873554082\n",
            "2021/05/07 19:57:15\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/pytorch_model.bin\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/cola/ce/cola-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/07 19:57:21\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/07 19:57:22\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/cola/default_experiment-1-0.arrow\n",
            "2021/05/07 19:57:22\tINFO\t__main__\tTest: matthews_correlation = 0.546773873554082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MzjOFPY1w1r"
      },
      "source": [
        "### 4.2 SST-2 task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acMDi9f3pd50",
        "outputId": "255fa223-dd32-4d02-a23a-e591cf4e0343"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/sst2/ce/bert_base_uncased.yaml \\\n",
        "  --task sst2 \\\n",
        "  --log log/glue/sst2/ce/bert_base_uncased.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 19:57:26.289365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/07 19:57:27\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/sst2/ce/bert_base_uncased.yaml', log='log/glue/sst2/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='sst2', test_only=False, world_size=1)\n",
            "2021/05/07 19:57:27\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 7.44M/7.44M [00:00<00:00, 28.4MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 68/68 [00:02<00:00, 22.80ba/s]\n",
            "100% 1/1 [00:00<00:00, 17.03ba/s]\n",
            "100% 2/2 [00:00<00:00, 16.50ba/s]\n",
            "2021/05/07 19:57:39\tINFO\t__main__\tStart training\n",
            "2021/05/07 19:57:39\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/07 19:57:39\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/07 19:57:39\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/07 19:57:42\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [   0/2105]  eta: 0:06:20  lr: 1.9996832937450518e-05  sample/s: 23.079135503058698  loss: 0.7150 (0.7150)  time: 0.1807  data: 0.0073  max mem: 1854\n",
            "2021/05/07 19:58:45\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 500/2105]  eta: 0:03:22  lr: 1.8413301662707842e-05  sample/s: 31.120036059230625  loss: 0.2213 (0.3550)  time: 0.1276  data: 0.0032  max mem: 3153\n",
            "2021/05/07 19:59:49\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [1000/2105]  eta: 0:02:20  lr: 1.6829770387965163e-05  sample/s: 30.686361873824836  loss: 0.2025 (0.2794)  time: 0.1327  data: 0.0031  max mem: 3165\n",
            "2021/05/07 20:00:53\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [1500/2105]  eta: 0:01:16  lr: 1.5246239113222487e-05  sample/s: 33.439133866428556  loss: 0.1641 (0.2463)  time: 0.1227  data: 0.0031  max mem: 3165\n",
            "2021/05/07 20:01:57\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [2000/2105]  eta: 0:00:13  lr: 1.3662707838479811e-05  sample/s: 33.31370715211879  loss: 0.1976 (0.2256)  time: 0.1295  data: 0.0031  max mem: 3165\n",
            "2021/05/07 20:02:11\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:04:28\n",
            "2021/05/07 20:02:12\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow\n",
            "2021/05/07 20:02:12\tINFO\t__main__\tValidation: accuracy = 0.9208715596330275\n",
            "2021/05/07 20:02:12\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/sst2/ce/sst2-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/sst2/ce/sst2-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:02:13\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [   0/2105]  eta: 0:04:49  lr: 1.3330166270783848e-05  sample/s: 30.6760940914036  loss: 0.0142 (0.0142)  time: 0.1377  data: 0.0073  max mem: 3165\n",
            "2021/05/07 20:03:18\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 500/2105]  eta: 0:03:27  lr: 1.1746634996041172e-05  sample/s: 30.252438808887543  loss: 0.0068 (0.1163)  time: 0.1317  data: 0.0033  max mem: 3165\n",
            "2021/05/07 20:04:23\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [1000/2105]  eta: 0:02:23  lr: 1.0163103721298497e-05  sample/s: 33.04455452369932  loss: 0.1271 (0.1205)  time: 0.1318  data: 0.0031  max mem: 3165\n",
            "2021/05/07 20:05:27\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [1500/2105]  eta: 0:01:18  lr: 8.57957244655582e-06  sample/s: 30.50924339796256  loss: 0.0863 (0.1224)  time: 0.1293  data: 0.0031  max mem: 3165\n",
            "2021/05/07 20:06:32\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [2000/2105]  eta: 0:00:13  lr: 6.996041171813144e-06  sample/s: 32.89630845308753  loss: 0.0642 (0.1215)  time: 0.1292  data: 0.0030  max mem: 3165\n",
            "2021/05/07 20:06:46\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:04:33\n",
            "2021/05/07 20:06:47\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow\n",
            "2021/05/07 20:06:47\tINFO\t__main__\tValidation: accuracy = 0.9243119266055045\n",
            "2021/05/07 20:06:47\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/sst2/ce/sst2-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/sst2/ce/sst2-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:06:48\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [   0/2105]  eta: 0:04:49  lr: 6.6634996041171816e-06  sample/s: 30.787102504298616  loss: 0.0373 (0.0373)  time: 0.1373  data: 0.0074  max mem: 3165\n",
            "2021/05/07 20:07:54\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 500/2105]  eta: 0:03:29  lr: 5.079968329374505e-06  sample/s: 33.380984405031455  loss: 0.1917 (0.1083)  time: 0.1302  data: 0.0029  max mem: 3165\n",
            "2021/05/07 20:08:58\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [1000/2105]  eta: 0:02:23  lr: 3.4964370546318295e-06  sample/s: 33.30458103308976  loss: 0.0001 (0.1423)  time: 0.1257  data: 0.0029  max mem: 3165\n",
            "2021/05/07 20:10:03\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [1500/2105]  eta: 0:01:18  lr: 1.9129057798891528e-06  sample/s: 33.136776074358785  loss: 0.1354 (0.1570)  time: 0.1260  data: 0.0031  max mem: 3165\n",
            "2021/05/07 20:11:07\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [2000/2105]  eta: 0:00:13  lr: 3.293745051464767e-07  sample/s: 29.95968878182406  loss: 0.0000 (0.1608)  time: 0.1263  data: 0.0030  max mem: 3165\n",
            "2021/05/07 20:11:20\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:04:32\n",
            "2021/05/07 20:11:21\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow\n",
            "2021/05/07 20:11:21\tINFO\t__main__\tValidation: accuracy = 0.9197247706422018\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/sst2/ce/sst2-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/sst2/ce/sst2-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/07 20:11:26\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/07 20:11:27\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/sst2/default_experiment-1-0.arrow\n",
            "2021/05/07 20:11:27\tINFO\t__main__\tTest: accuracy = 0.9243119266055045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjKsN2wz10Lb"
      },
      "source": [
        "### 4.3 MRPC task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTHMMfEWpsdN",
        "outputId": "b71e426a-9943-4387-8a8d-80084a79683f"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/mrpc/ce/bert_base_uncased.yaml \\\n",
        "  --task mrpc \\\n",
        "  --log log/glue/mrpc/ce/bert_base_uncased.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 20:11:31.598741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/07 20:11:32\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/mrpc/ce/bert_base_uncased.yaml', log='log/glue/mrpc/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='mrpc', test_only=False, world_size=1)\n",
            "2021/05/07 20:11:32\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 6.22kB [00:00, 5.58MB/s]\n",
            "Downloading: 1.05MB [00:00, 7.00MB/s]\n",
            "Downloading: 441kB [00:00, 4.08MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 4/4 [00:00<00:00,  8.57ba/s]\n",
            "100% 1/1 [00:00<00:00, 16.80ba/s]\n",
            "100% 2/2 [00:00<00:00,  9.10ba/s]\n",
            "2021/05/07 20:11:42\tINFO\t__main__\tStart training\n",
            "2021/05/07 20:11:42\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/07 20:11:42\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/07 20:11:42\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/07 20:11:45\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [  0/115]  eta: 0:00:28  lr: 1.996521739130435e-05  sample/s: 16.421140000234907  loss: 0.7215 (0.7215)  time: 0.2495  data: 0.0059  max mem: 2057\n",
            "2021/05/07 20:11:55\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 50/115]  eta: 0:00:12  lr: 1.822608695652174e-05  sample/s: 22.52259479719589  loss: 0.6212 (0.6465)  time: 0.1986  data: 0.0042  max mem: 3895\n",
            "2021/05/07 20:12:05\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [100/115]  eta: 0:00:02  lr: 1.6486956521739132e-05  sample/s: 19.1736762768096  loss: 0.6443 (0.6439)  time: 0.2010  data: 0.0043  max mem: 3895\n",
            "2021/05/07 20:12:08\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:22\n",
            "2021/05/07 20:12:08\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "2021/05/07 20:12:08\tINFO\t__main__\tValidation: accuracy = 0.6838235294117647, f1 = 0.8122270742358079\n",
            "2021/05/07 20:12:08\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:12:10\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [  0/115]  eta: 0:00:21  lr: 1.596521739130435e-05  sample/s: 22.25862499004962  loss: 0.5809 (0.5809)  time: 0.1850  data: 0.0053  max mem: 3895\n",
            "2021/05/07 20:12:20\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 50/115]  eta: 0:00:13  lr: 1.4226086956521742e-05  sample/s: 20.43091791204723  loss: 0.5760 (0.6092)  time: 0.2076  data: 0.0043  max mem: 3895\n",
            "2021/05/07 20:12:30\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [100/115]  eta: 0:00:03  lr: 1.2486956521739131e-05  sample/s: 19.172405439101514  loss: 0.5922 (0.5962)  time: 0.2071  data: 0.0045  max mem: 3895\n",
            "2021/05/07 20:12:33\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:23\n",
            "2021/05/07 20:12:34\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "2021/05/07 20:12:34\tINFO\t__main__\tValidation: accuracy = 0.7254901960784313, f1 = 0.8282208588957055\n",
            "2021/05/07 20:12:34\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:12:35\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [  0/115]  eta: 0:00:22  lr: 1.196521739130435e-05  sample/s: 21.25950659116051  loss: 0.5793 (0.5793)  time: 0.1936  data: 0.0054  max mem: 3895\n",
            "2021/05/07 20:12:45\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 50/115]  eta: 0:00:13  lr: 1.022608695652174e-05  sample/s: 20.95166829429116  loss: 0.5384 (0.5464)  time: 0.2073  data: 0.0047  max mem: 3895\n",
            "2021/05/07 20:12:55\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [100/115]  eta: 0:00:03  lr: 8.48695652173913e-06  sample/s: 19.4041497999121  loss: 0.5446 (0.5397)  time: 0.1963  data: 0.0044  max mem: 3895\n",
            "2021/05/07 20:12:58\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:23\n",
            "2021/05/07 20:12:59\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "2021/05/07 20:12:59\tINFO\t__main__\tValidation: accuracy = 0.7475490196078431, f1 = 0.8291873963515755\n",
            "2021/05/07 20:12:59\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:13:00\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [  0/115]  eta: 0:00:21  lr: 7.965217391304349e-06  sample/s: 21.718219535401524  loss: 0.5355 (0.5355)  time: 0.1892  data: 0.0051  max mem: 3895\n",
            "2021/05/07 20:13:10\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [ 50/115]  eta: 0:00:12  lr: 6.226086956521739e-06  sample/s: 19.79624186573372  loss: 0.5103 (0.5185)  time: 0.2027  data: 0.0044  max mem: 3895\n",
            "2021/05/07 20:13:20\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [100/115]  eta: 0:00:02  lr: 4.486956521739131e-06  sample/s: 19.359346242475084  loss: 0.4742 (0.5014)  time: 0.2018  data: 0.0045  max mem: 3895\n",
            "2021/05/07 20:13:23\tINFO\ttorchdistill.misc.log\tEpoch: [3] Total time: 0:00:22\n",
            "2021/05/07 20:13:24\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "2021/05/07 20:13:24\tINFO\t__main__\tValidation: accuracy = 0.7671568627450981, f1 = 0.8450244698205546\n",
            "2021/05/07 20:13:24\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:13:25\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [  0/115]  eta: 0:00:24  lr: 3.965217391304348e-06  sample/s: 19.83415516477021  loss: 0.3788 (0.3788)  time: 0.2115  data: 0.0098  max mem: 3895\n",
            "2021/05/07 20:13:35\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [ 50/115]  eta: 0:00:13  lr: 2.2260869565217395e-06  sample/s: 20.648601680469557  loss: 0.4873 (0.4531)  time: 0.2032  data: 0.0045  max mem: 3895\n",
            "2021/05/07 20:13:46\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [100/115]  eta: 0:00:03  lr: 4.869565217391305e-07  sample/s: 19.069005337479656  loss: 0.4652 (0.4554)  time: 0.2051  data: 0.0046  max mem: 3895\n",
            "2021/05/07 20:13:48\tINFO\ttorchdistill.misc.log\tEpoch: [4] Total time: 0:00:23\n",
            "2021/05/07 20:13:49\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "2021/05/07 20:13:49\tINFO\t__main__\tValidation: accuracy = 0.7818627450980392, f1 = 0.8524046434494196\n",
            "2021/05/07 20:13:49\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/pytorch_model.bin\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/mrpc/ce/mrpc-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/07 20:13:55\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/07 20:13:56\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mrpc/default_experiment-1-0.arrow\n",
            "2021/05/07 20:13:56\tINFO\t__main__\tTest: accuracy = 0.7818627450980392, f1 = 0.8524046434494196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCFuvFRv14Ky"
      },
      "source": [
        "### 4.4 STS-B task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDUYbl2fpxu8",
        "outputId": "1002629d-154f-497a-b537-a3284aa6ebde"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/stsb/ce/bert_base_uncased.yaml \\\n",
        "  --task stsb \\\n",
        "  --log log/glue/stsb/ce/bert_base_uncased.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 20:14:00.153528: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/07 20:14:01\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/stsb/ce/bert_base_uncased.yaml', log='log/glue/stsb/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='stsb', test_only=False, world_size=1)\n",
            "2021/05/07 20:14:01\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 803k/803k [00:00<00:00, 5.26MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 6/6 [00:00<00:00, 12.88ba/s]\n",
            "100% 2/2 [00:00<00:00,  8.44ba/s]\n",
            "100% 2/2 [00:00<00:00, 19.33ba/s]\n",
            "2021/05/07 20:14:09\tINFO\t__main__\tStart training\n",
            "2021/05/07 20:14:09\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/07 20:14:09\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/07 20:14:09\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "2021/05/07 20:14:12\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [  0/180]  eta: 0:00:29  lr: 1.9962962962962963e-05  sample/s: 25.2868849617544  loss: 5.5916 (5.5916)  time: 0.1636  data: 0.0054  max mem: 1721\n",
            "2021/05/07 20:14:21\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 50/180]  eta: 0:00:22  lr: 1.8111111111111112e-05  sample/s: 25.191128488566765  loss: 2.6007 (3.6315)  time: 0.1746  data: 0.0038  max mem: 3900\n",
            "2021/05/07 20:14:30\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [100/180]  eta: 0:00:14  lr: 1.625925925925926e-05  sample/s: 23.002402079608647  loss: 0.9923 (2.5119)  time: 0.1781  data: 0.0037  max mem: 4470\n",
            "2021/05/07 20:14:39\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [150/180]  eta: 0:00:05  lr: 1.4407407407407407e-05  sample/s: 24.889131524838373  loss: 0.6842 (1.9295)  time: 0.1847  data: 0.0038  max mem: 4470\n",
            "2021/05/07 20:14:44\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:32\n",
            "2021/05/07 20:14:46\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/stsb/default_experiment-1-0.arrow\n",
            "2021/05/07 20:14:46\tINFO\t__main__\tValidation: pearson = 0.8652237689820927, spearmanr = 0.8696583870994744\n",
            "2021/05/07 20:14:46\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:14:48\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [  0/180]  eta: 0:00:29  lr: 1.3296296296296298e-05  sample/s: 24.941191161801235  loss: 0.4750 (0.4750)  time: 0.1653  data: 0.0049  max mem: 4470\n",
            "2021/05/07 20:14:57\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 50/180]  eta: 0:00:23  lr: 1.1444444444444444e-05  sample/s: 21.14416208341105  loss: 0.4613 (0.4648)  time: 0.1849  data: 0.0039  max mem: 4470\n",
            "2021/05/07 20:15:06\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [100/180]  eta: 0:00:14  lr: 9.592592592592593e-06  sample/s: 20.724560950572737  loss: 0.4365 (0.4692)  time: 0.1793  data: 0.0038  max mem: 4470\n",
            "2021/05/07 20:15:15\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [150/180]  eta: 0:00:05  lr: 7.74074074074074e-06  sample/s: 32.87542301308768  loss: 0.4818 (0.4657)  time: 0.1800  data: 0.0038  max mem: 4470\n",
            "2021/05/07 20:15:20\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:32\n",
            "2021/05/07 20:15:22\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/stsb/default_experiment-1-0.arrow\n",
            "2021/05/07 20:15:22\tINFO\t__main__\tValidation: pearson = 0.8789010150252042, spearmanr = 0.8779071865584523\n",
            "2021/05/07 20:15:22\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:15:23\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [  0/180]  eta: 0:00:34  lr: 6.62962962962963e-06  sample/s: 21.717404248929483  loss: 0.4171 (0.4171)  time: 0.1911  data: 0.0069  max mem: 4470\n",
            "2021/05/07 20:15:32\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 50/180]  eta: 0:00:23  lr: 4.777777777777778e-06  sample/s: 24.83945835665195  loss: 0.2964 (0.3148)  time: 0.1772  data: 0.0039  max mem: 4470\n",
            "2021/05/07 20:15:41\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [100/180]  eta: 0:00:14  lr: 2.9259259259259257e-06  sample/s: 25.168114298401154  loss: 0.3584 (0.3237)  time: 0.1831  data: 0.0039  max mem: 4470\n",
            "2021/05/07 20:15:50\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [150/180]  eta: 0:00:05  lr: 1.074074074074074e-06  sample/s: 27.049293344898636  loss: 0.3344 (0.3195)  time: 0.1818  data: 0.0038  max mem: 4470\n",
            "2021/05/07 20:15:55\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:32\n",
            "2021/05/07 20:15:57\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/stsb/default_experiment-1-0.arrow\n",
            "2021/05/07 20:15:57\tINFO\t__main__\tValidation: pearson = 0.8795903033047023, spearmanr = 0.8784151483476638\n",
            "2021/05/07 20:15:57\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/pytorch_model.bin\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/stsb/ce/stsb-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/07 20:16:03\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/07 20:16:05\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/stsb/default_experiment-1-0.arrow\n",
            "2021/05/07 20:16:05\tINFO\t__main__\tTest: pearson = 0.8795903033047023, spearmanr = 0.8784151483476638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxS1o7i118Eq"
      },
      "source": [
        "### 4.5 QQP task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtA-gDQYp2Hf",
        "outputId": "0df3af66-9bf9-4be5-f931-48e5f66590de"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/qqp/ce/bert_base_uncased.yaml \\\n",
        "  --task qqp \\\n",
        "  --log log/glue/qqp/ce/bert_base_uncased.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 20:16:09.775156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/07 20:16:10\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/qqp/ce/bert_base_uncased.yaml', log='log/glue/qqp/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='qqp', test_only=False, world_size=1)\n",
            "2021/05/07 20:16:10\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"qqp\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 41.7M/41.7M [00:00<00:00, 54.7MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 364/364 [00:30<00:00, 12.03ba/s]\n",
            "100% 41/41 [00:03<00:00, 12.33ba/s]\n",
            "100% 391/391 [00:32<00:00, 11.94ba/s]\n",
            "2021/05/07 20:17:47\tINFO\t__main__\tStart training\n",
            "2021/05/07 20:17:47\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/07 20:17:47\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/07 20:17:47\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/07 20:17:51\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [    0/11371]  eta: 0:39:10  lr: 1.9999413713247152e-05  sample/s: 22.03373901577683  loss: 0.6879 (0.6879)  time: 0.2067  data: 0.0252  max mem: 1854\n",
            "2021/05/07 20:20:46\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 1000/11371]  eta: 0:30:20  lr: 1.941312696039633e-05  sample/s: 25.045330912977665  loss: 0.3074 (0.4223)  time: 0.1745  data: 0.0037  max mem: 4435\n",
            "2021/05/07 20:23:46\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 2000/11371]  eta: 0:27:43  lr: 1.8826840207545512e-05  sample/s: 29.975854534019664  loss: 0.3626 (0.3790)  time: 0.1840  data: 0.0039  max mem: 4435\n",
            "2021/05/07 20:26:45\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 3000/11371]  eta: 0:24:50  lr: 1.824055345469469e-05  sample/s: 17.812136770648202  loss: 0.3413 (0.3551)  time: 0.1770  data: 0.0037  max mem: 4435\n",
            "2021/05/07 20:29:44\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 4000/11371]  eta: 0:21:54  lr: 1.7654266701843875e-05  sample/s: 26.525787881806266  loss: 0.2489 (0.3404)  time: 0.1728  data: 0.0038  max mem: 4435\n",
            "2021/05/07 20:32:42\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 5000/11371]  eta: 0:18:55  lr: 1.7067979948993053e-05  sample/s: 21.535203963751187  loss: 0.2792 (0.3296)  time: 0.1844  data: 0.0038  max mem: 4435\n",
            "2021/05/07 20:35:39\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 6000/11371]  eta: 0:15:55  lr: 1.6481693196142235e-05  sample/s: 25.161584273049993  loss: 0.2112 (0.3219)  time: 0.1726  data: 0.0040  max mem: 4435\n",
            "2021/05/07 20:38:36\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 7000/11371]  eta: 0:12:57  lr: 1.5895406443291413e-05  sample/s: 29.905591027546944  loss: 0.2033 (0.3148)  time: 0.1809  data: 0.0040  max mem: 4435\n",
            "2021/05/07 20:41:33\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 8000/11371]  eta: 0:09:59  lr: 1.5309119690440595e-05  sample/s: 26.731529797536403  loss: 0.2399 (0.3086)  time: 0.1877  data: 0.0040  max mem: 4435\n",
            "2021/05/07 20:44:30\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 9000/11371]  eta: 0:07:01  lr: 1.4722832937589777e-05  sample/s: 19.751820400493994  loss: 0.1892 (0.3036)  time: 0.1645  data: 0.0036  max mem: 4435\n",
            "2021/05/07 20:47:26\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [10000/11371]  eta: 0:04:03  lr: 1.4136546184738957e-05  sample/s: 27.05300075303026  loss: 0.2623 (0.2993)  time: 0.1829  data: 0.0040  max mem: 4435\n",
            "2021/05/07 20:50:23\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [11000/11371]  eta: 0:01:05  lr: 1.3550259431888138e-05  sample/s: 25.238688092523113  loss: 0.2053 (0.2950)  time: 0.1703  data: 0.0040  max mem: 4435\n",
            "2021/05/07 20:51:28\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:33:37\n",
            "2021/05/07 20:52:33\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qqp/default_experiment-1-0.arrow\n",
            "2021/05/07 20:52:33\tINFO\t__main__\tValidation: accuracy = 0.8924313628493693, f1 = 0.8557497761119772\n",
            "2021/05/07 20:52:33\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 20:52:34\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [    0/11371]  eta: 0:48:25  lr: 1.3332747046580484e-05  sample/s: 17.38281592397538  loss: 0.1932 (0.1932)  time: 0.2555  data: 0.0254  max mem: 4435\n",
            "2021/05/07 20:55:31\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 1000/11371]  eta: 0:30:32  lr: 1.2746460293729664e-05  sample/s: 25.143370969724035  loss: 0.1890 (0.2006)  time: 0.1862  data: 0.0040  max mem: 4435\n",
            "2021/05/07 20:58:27\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 2000/11371]  eta: 0:27:34  lr: 1.2160173540878845e-05  sample/s: 22.894266340799085  loss: 0.1386 (0.1957)  time: 0.1785  data: 0.0038  max mem: 4435\n",
            "2021/05/07 21:01:24\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 3000/11371]  eta: 0:24:38  lr: 1.1573886788028025e-05  sample/s: 21.432260009555417  loss: 0.1765 (0.1980)  time: 0.1738  data: 0.0038  max mem: 4435\n",
            "2021/05/07 21:04:21\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 4000/11371]  eta: 0:21:43  lr: 1.0987600035177207e-05  sample/s: 21.45563969005651  loss: 0.1676 (0.1979)  time: 0.1712  data: 0.0039  max mem: 4435\n",
            "2021/05/07 21:07:18\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 5000/11371]  eta: 0:18:46  lr: 1.0401313282326387e-05  sample/s: 16.472847202244136  loss: 0.1768 (0.1970)  time: 0.1736  data: 0.0037  max mem: 4435\n",
            "2021/05/07 21:10:16\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 6000/11371]  eta: 0:15:50  lr: 9.815026529475568e-06  sample/s: 25.231210682483656  loss: 0.1503 (0.1963)  time: 0.1804  data: 0.0037  max mem: 4435\n",
            "2021/05/07 21:13:11\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 7000/11371]  eta: 0:12:52  lr: 9.228739776624748e-06  sample/s: 26.77111367484135  loss: 0.1589 (0.1945)  time: 0.1712  data: 0.0038  max mem: 4435\n",
            "2021/05/07 21:16:09\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 8000/11371]  eta: 0:09:56  lr: 8.642453023773928e-06  sample/s: 21.58968670456856  loss: 0.1765 (0.1932)  time: 0.1742  data: 0.0037  max mem: 4435\n",
            "2021/05/07 21:19:06\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 9000/11371]  eta: 0:06:59  lr: 8.05616627092311e-06  sample/s: 27.127012024854924  loss: 0.1345 (0.1927)  time: 0.1727  data: 0.0039  max mem: 4435\n",
            "2021/05/07 21:22:02\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [10000/11371]  eta: 0:04:02  lr: 7.469879518072289e-06  sample/s: 23.05969025064737  loss: 0.1842 (0.1919)  time: 0.1727  data: 0.0037  max mem: 4435\n",
            "2021/05/07 21:24:59\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [11000/11371]  eta: 0:01:05  lr: 6.883592765221471e-06  sample/s: 21.359841926218657  loss: 0.1420 (0.1907)  time: 0.1792  data: 0.0040  max mem: 4435\n",
            "2021/05/07 21:26:04\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:33:29\n",
            "2021/05/07 21:27:09\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qqp/default_experiment-1-0.arrow\n",
            "2021/05/07 21:27:09\tINFO\t__main__\tValidation: accuracy = 0.8988622310165718, f1 = 0.862679249084864\n",
            "2021/05/07 21:27:09\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 21:27:11\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [    0/11371]  eta: 0:44:00  lr: 6.666080379913816e-06  sample/s: 19.243192651963867  loss: 0.1835 (0.1835)  time: 0.2322  data: 0.0243  max mem: 4435\n",
            "2021/05/07 21:30:07\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 1000/11371]  eta: 0:30:26  lr: 6.079793627062997e-06  sample/s: 27.307688927157105  loss: 0.0660 (0.1234)  time: 0.1826  data: 0.0040  max mem: 4435\n",
            "2021/05/07 21:33:05\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 2000/11371]  eta: 0:27:41  lr: 5.493506874212178e-06  sample/s: 21.607203939912218  loss: 0.0794 (0.1193)  time: 0.1817  data: 0.0039  max mem: 4435\n",
            "2021/05/07 21:36:03\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 3000/11371]  eta: 0:24:45  lr: 4.9072201213613585e-06  sample/s: 26.79142021283391  loss: 0.1356 (0.1174)  time: 0.1928  data: 0.0040  max mem: 4435\n",
            "2021/05/07 21:39:01\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 4000/11371]  eta: 0:21:49  lr: 4.3209333685105384e-06  sample/s: 24.965426521313418  loss: 0.1200 (0.1169)  time: 0.1784  data: 0.0038  max mem: 4435\n",
            "2021/05/07 21:42:00\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 5000/11371]  eta: 0:18:53  lr: 3.7346466156597192e-06  sample/s: 22.76278007071472  loss: 0.0918 (0.1163)  time: 0.1762  data: 0.0039  max mem: 4435\n",
            "2021/05/07 21:44:59\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 6000/11371]  eta: 0:15:55  lr: 3.1483598628089e-06  sample/s: 27.09546842724919  loss: 0.0674 (0.1155)  time: 0.1775  data: 0.0041  max mem: 4435\n",
            "2021/05/07 21:47:57\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 7000/11371]  eta: 0:12:57  lr: 2.562073109958081e-06  sample/s: 26.604994576628123  loss: 0.0437 (0.1151)  time: 0.1683  data: 0.0037  max mem: 4435\n",
            "2021/05/07 21:50:54\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 8000/11371]  eta: 0:09:59  lr: 1.9757863571072612e-06  sample/s: 25.048097939683487  loss: 0.0559 (0.1140)  time: 0.1825  data: 0.0038  max mem: 4435\n",
            "2021/05/07 21:53:53\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 9000/11371]  eta: 0:07:01  lr: 1.3894996042564418e-06  sample/s: 16.137198567610017  loss: 0.1063 (0.1138)  time: 0.1749  data: 0.0038  max mem: 4435\n",
            "2021/05/07 21:56:50\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [10000/11371]  eta: 0:04:03  lr: 8.032128514056225e-07  sample/s: 26.97501101367625  loss: 0.0810 (0.1132)  time: 0.1741  data: 0.0036  max mem: 4435\n",
            "2021/05/07 21:59:48\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [11000/11371]  eta: 0:01:06  lr: 2.169260985548032e-07  sample/s: 21.363160133140802  loss: 0.0460 (0.1128)  time: 0.1776  data: 0.0039  max mem: 4435\n",
            "2021/05/07 22:00:55\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:33:43\n",
            "2021/05/07 22:02:01\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qqp/default_experiment-1-0.arrow\n",
            "2021/05/07 22:02:01\tINFO\t__main__\tValidation: accuracy = 0.8991343062082612, f1 = 0.8642838125665603\n",
            "2021/05/07 22:02:01\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/pytorch_model.bin\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"qqp\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/qqp/ce/qqp-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/07 22:02:07\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/07 22:03:14\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qqp/default_experiment-1-0.arrow\n",
            "2021/05/07 22:03:14\tINFO\t__main__\tTest: accuracy = 0.8991343062082612, f1 = 0.8642838125665603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGATNCSI1_vr"
      },
      "source": [
        "### 4.6 MNLI task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMnPiXycp8-B",
        "outputId": "f360a2fe-b361-413e-d8ab-7cb148a4eb79"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/mnli/ce/bert_base_uncased.yaml \\\n",
        "  --task mnli \\\n",
        "  --log log/glue/mnli/ce/bert_base_uncased.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-07 22:03:18.224213: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/07 22:03:19\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/mnli/ce/bert_base_uncased.yaml', log='log/glue/mnli/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='mnli', test_only=False, world_size=1)\n",
            "2021/05/07 22:03:19\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 313M/313M [00:05<00:00, 52.5MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 393/393 [00:42<00:00,  9.21ba/s]\n",
            "100% 10/10 [00:00<00:00, 10.00ba/s]\n",
            "100% 10/10 [00:01<00:00,  8.78ba/s]\n",
            "100% 10/10 [00:00<00:00, 10.32ba/s]\n",
            "100% 10/10 [00:01<00:00,  8.88ba/s]\n",
            "2021/05/07 22:04:47\tINFO\t__main__\tStart training\n",
            "2021/05/07 22:04:47\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/07 22:04:47\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/07 22:04:47\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/07 22:04:51\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [    0/12272]  eta: 0:54:29  lr: 1.9999456757931336e-05  sample/s: 16.829522154857298  loss: 1.2475 (1.2475)  time: 0.2664  data: 0.0288  max mem: 1895\n",
            "2021/05/07 22:08:17\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 1000/12272]  eta: 0:38:47  lr: 1.945621468926554e-05  sample/s: 15.287662595051188  loss: 0.6553 (0.8945)  time: 0.2175  data: 0.0047  max mem: 4450\n",
            "2021/05/07 22:11:50\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 2000/12272]  eta: 0:35:53  lr: 1.891297262059974e-05  sample/s: 21.47486396782588  loss: 0.5742 (0.7690)  time: 0.2176  data: 0.0045  max mem: 4450\n",
            "2021/05/07 22:15:26\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 3000/12272]  eta: 0:32:42  lr: 1.8369730551933943e-05  sample/s: 14.909873609410242  loss: 0.4953 (0.7054)  time: 0.2182  data: 0.0044  max mem: 4450\n",
            "2021/05/07 22:19:01\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 4000/12272]  eta: 0:29:18  lr: 1.7826488483268146e-05  sample/s: 18.829393992449035  loss: 0.5564 (0.6671)  time: 0.2199  data: 0.0047  max mem: 4450\n",
            "2021/05/07 22:22:38\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 5000/12272]  eta: 0:25:52  lr: 1.728324641460235e-05  sample/s: 14.955687982207088  loss: 0.5767 (0.6414)  time: 0.2093  data: 0.0045  max mem: 4450\n",
            "2021/05/07 22:26:15\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 6000/12272]  eta: 0:22:21  lr: 1.674000434593655e-05  sample/s: 21.233649401866035  loss: 0.6007 (0.6199)  time: 0.2137  data: 0.0045  max mem: 4450\n",
            "2021/05/07 22:29:48\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 7000/12272]  eta: 0:18:47  lr: 1.6196762277270753e-05  sample/s: 17.775958212149625  loss: 0.4044 (0.6032)  time: 0.2240  data: 0.0047  max mem: 4450\n",
            "2021/05/07 22:33:22\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 8000/12272]  eta: 0:15:13  lr: 1.5653520208604957e-05  sample/s: 25.156415500602776  loss: 0.4456 (0.5892)  time: 0.2047  data: 0.0043  max mem: 4450\n",
            "2021/05/07 22:36:56\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 9000/12272]  eta: 0:11:39  lr: 1.5110278139939158e-05  sample/s: 24.90597991158246  loss: 0.4227 (0.5759)  time: 0.2160  data: 0.0044  max mem: 4450\n",
            "2021/05/07 22:40:30\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [10000/12272]  eta: 0:08:05  lr: 1.4567036071273362e-05  sample/s: 19.808536085736314  loss: 0.4536 (0.5662)  time: 0.2153  data: 0.0045  max mem: 4450\n",
            "2021/05/07 22:44:04\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [11000/12272]  eta: 0:04:32  lr: 1.4023794002607562e-05  sample/s: 21.099647610112406  loss: 0.4070 (0.5566)  time: 0.2129  data: 0.0044  max mem: 4450\n",
            "2021/05/07 22:47:38\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [12000/12272]  eta: 0:00:58  lr: 1.3480551933941765e-05  sample/s: 17.721356722082383  loss: 0.3461 (0.5474)  time: 0.2169  data: 0.0044  max mem: 4450\n",
            "2021/05/07 22:48:36\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:43:45\n",
            "2021/05/07 22:48:57\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "2021/05/07 22:48:57\tINFO\t__main__\tValidation: accuracy = 0.8269994905756495\n",
            "2021/05/07 22:48:57\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 22:48:58\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [    0/12272]  eta: 0:46:13  lr: 1.333279009126467e-05  sample/s: 21.51468511919036  loss: 0.3045 (0.3045)  time: 0.2260  data: 0.0400  max mem: 4450\n",
            "2021/05/07 22:52:31\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 1000/12272]  eta: 0:40:02  lr: 1.2789548022598873e-05  sample/s: 21.516699370294845  loss: 0.3553 (0.3753)  time: 0.2273  data: 0.0045  max mem: 4450\n",
            "2021/05/07 22:56:06\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 2000/12272]  eta: 0:36:34  lr: 1.2246305953933073e-05  sample/s: 15.447282235900175  loss: 0.3334 (0.3735)  time: 0.2233  data: 0.0045  max mem: 4450\n",
            "2021/05/07 22:59:39\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 3000/12272]  eta: 0:33:01  lr: 1.1703063885267276e-05  sample/s: 22.824529421047764  loss: 0.3176 (0.3713)  time: 0.2110  data: 0.0043  max mem: 4450\n",
            "2021/05/07 23:03:12\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 4000/12272]  eta: 0:29:25  lr: 1.115982181660148e-05  sample/s: 16.071346394985827  loss: 0.3426 (0.3730)  time: 0.2159  data: 0.0046  max mem: 4450\n",
            "2021/05/07 23:06:46\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 5000/12272]  eta: 0:25:53  lr: 1.061657974793568e-05  sample/s: 16.4946462156792  loss: 0.3660 (0.3734)  time: 0.2191  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:10:21\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 6000/12272]  eta: 0:22:20  lr: 1.0073337679269883e-05  sample/s: 15.144256278281718  loss: 0.4082 (0.3739)  time: 0.2123  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:13:55\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 7000/12272]  eta: 0:18:47  lr: 9.530095610604087e-06  sample/s: 21.506521608154863  loss: 0.3622 (0.3743)  time: 0.2222  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:17:29\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 8000/12272]  eta: 0:15:13  lr: 8.986853541938288e-06  sample/s: 21.02105085106783  loss: 0.3902 (0.3744)  time: 0.2154  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:21:02\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 9000/12272]  eta: 0:11:39  lr: 8.44361147327249e-06  sample/s: 22.894797460684828  loss: 0.3203 (0.3747)  time: 0.2077  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:24:35\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [10000/12272]  eta: 0:08:05  lr: 7.900369404606693e-06  sample/s: 18.91816591887293  loss: 0.4424 (0.3737)  time: 0.2057  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:28:08\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [11000/12272]  eta: 0:04:31  lr: 7.357127335940896e-06  sample/s: 18.67706581140158  loss: 0.3000 (0.3730)  time: 0.2078  data: 0.0043  max mem: 4450\n",
            "2021/05/07 23:31:42\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [12000/12272]  eta: 0:00:58  lr: 6.813885267275099e-06  sample/s: 21.376361567639126  loss: 0.3522 (0.3728)  time: 0.2072  data: 0.0047  max mem: 4450\n",
            "2021/05/07 23:32:40\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:43:42\n",
            "2021/05/07 23:33:01\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "2021/05/07 23:33:01\tINFO\t__main__\tValidation: accuracy = 0.8317880794701987\n",
            "2021/05/07 23:33:01\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/07 23:33:03\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [    0/12272]  eta: 0:43:16  lr: 6.666123424598001e-06  sample/s: 21.689776964168438  loss: 0.1988 (0.1988)  time: 0.2116  data: 0.0272  max mem: 4450\n",
            "2021/05/07 23:36:36\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 1000/12272]  eta: 0:40:06  lr: 6.122881355932204e-06  sample/s: 19.745845975674918  loss: 0.3094 (0.2818)  time: 0.2154  data: 0.0046  max mem: 4450\n",
            "2021/05/07 23:40:11\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 2000/12272]  eta: 0:36:37  lr: 5.579639287266406e-06  sample/s: 14.759243652384129  loss: 0.2872 (0.2868)  time: 0.2037  data: 0.0046  max mem: 4450\n",
            "2021/05/07 23:43:46\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 3000/12272]  eta: 0:33:08  lr: 5.0363972186006095e-06  sample/s: 16.411405765487288  loss: 0.1976 (0.2831)  time: 0.2139  data: 0.0043  max mem: 4450\n",
            "2021/05/07 23:47:20\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 4000/12272]  eta: 0:29:33  lr: 4.493155149934811e-06  sample/s: 24.625985459744218  loss: 0.3089 (0.2827)  time: 0.2162  data: 0.0043  max mem: 4450\n",
            "2021/05/07 23:50:55\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 5000/12272]  eta: 0:25:59  lr: 3.949913081269014e-06  sample/s: 16.415596817702824  loss: 0.2012 (0.2817)  time: 0.2166  data: 0.0042  max mem: 4450\n",
            "2021/05/07 23:54:28\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 6000/12272]  eta: 0:22:23  lr: 3.4066710126032164e-06  sample/s: 21.22575143784681  loss: 0.2323 (0.2822)  time: 0.2200  data: 0.0044  max mem: 4450\n",
            "2021/05/07 23:58:05\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 7000/12272]  eta: 0:18:51  lr: 2.8634289439374186e-06  sample/s: 18.61962821153099  loss: 0.2026 (0.2818)  time: 0.2023  data: 0.0043  max mem: 4450\n",
            "2021/05/08 00:01:39\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 8000/12272]  eta: 0:15:16  lr: 2.320186875271621e-06  sample/s: 22.633131337114243  loss: 0.2654 (0.2814)  time: 0.2067  data: 0.0043  max mem: 4450\n",
            "2021/05/08 00:05:14\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 9000/12272]  eta: 0:11:41  lr: 1.7769448066058238e-06  sample/s: 15.032970498948055  loss: 0.2993 (0.2808)  time: 0.2090  data: 0.0043  max mem: 4450\n",
            "2021/05/08 00:08:49\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [10000/12272]  eta: 0:08:07  lr: 1.2337027379400262e-06  sample/s: 16.18748956756407  loss: 0.3048 (0.2799)  time: 0.2269  data: 0.0046  max mem: 4450\n",
            "2021/05/08 00:12:25\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [11000/12272]  eta: 0:04:33  lr: 6.904606692742287e-07  sample/s: 22.848090819208654  loss: 0.2206 (0.2800)  time: 0.2267  data: 0.0044  max mem: 4450\n",
            "2021/05/08 00:15:59\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [12000/12272]  eta: 0:00:58  lr: 1.4721860060843112e-07  sample/s: 14.924038449563858  loss: 0.2385 (0.2799)  time: 0.2300  data: 0.0043  max mem: 4450\n",
            "2021/05/08 00:16:57\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:43:54\n",
            "2021/05/08 00:17:18\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:17:18\tINFO\t__main__\tValidation: accuracy = 0.832603158430973\n",
            "2021/05/08 00:17:18\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/pytorch_model.bin\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"mnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/mnli/ce/mnli-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/08 00:17:24\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/08 00:17:45\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:17:45\tINFO\t__main__\tTest: accuracy = 0.832603158430973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNMwSfQx2DN_"
      },
      "source": [
        "### 4.7 QNLI task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm6AEL9cqAnd",
        "outputId": "b71a098e-a562-4809-a665-d06bf067039d"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/qnli/ce/bert_base_uncased.yaml \\\n",
        "  --task qnli \\\n",
        "  --log log/glue/qnli/ce/bert_base_uncased.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 00:17:49.801334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/08 00:17:50\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/qnli/ce/bert_base_uncased.yaml', log='log/glue/qnli/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='qnli', test_only=False, world_size=1)\n",
            "2021/05/08 00:17:50\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/qnli (download: 10.14 MiB, generated: 27.11 MiB, post-processed: Unknown size, total: 37.24 MiB) to /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 10.6M/10.6M [00:00<00:00, 27.7MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 105/105 [00:13<00:00,  7.86ba/s]\n",
            "100% 6/6 [00:00<00:00,  8.96ba/s]\n",
            "100% 6/6 [00:00<00:00,  8.77ba/s]\n",
            "2021/05/08 00:18:16\tINFO\t__main__\tStart training\n",
            "2021/05/08 00:18:16\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/08 00:18:16\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/08 00:18:16\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/08 00:18:20\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [   0/3274]  eta: 0:18:12  lr: 1.9997963754836084e-05  sample/s: 12.345928810009662  loss: 0.7953 (0.7953)  time: 0.3338  data: 0.0098  max mem: 3175\n",
            "2021/05/08 00:20:13\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 500/3274]  eta: 0:10:29  lr: 1.8979841172877215e-05  sample/s: 18.74457121661058  loss: 0.4119 (0.5122)  time: 0.2330  data: 0.0045  max mem: 4464\n",
            "2021/05/08 00:22:07\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [1000/3274]  eta: 0:08:37  lr: 1.796171859091835e-05  sample/s: 16.40004770292806  loss: 0.3525 (0.4463)  time: 0.2290  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:24:01\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [1500/3274]  eta: 0:06:43  lr: 1.694359600895948e-05  sample/s: 17.47231719496739  loss: 0.3299 (0.4112)  time: 0.2339  data: 0.0045  max mem: 4464\n",
            "2021/05/08 00:25:53\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [2000/3274]  eta: 0:04:49  lr: 1.5925473427000613e-05  sample/s: 18.58870377120799  loss: 0.3412 (0.3896)  time: 0.2188  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:27:49\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [2500/3274]  eta: 0:02:56  lr: 1.4907350845041744e-05  sample/s: 17.5300254112612  loss: 0.2618 (0.3750)  time: 0.2411  data: 0.0046  max mem: 4464\n",
            "2021/05/08 00:29:44\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [3000/3274]  eta: 0:01:02  lr: 1.3889228263082878e-05  sample/s: 18.660716788311426  loss: 0.2850 (0.3620)  time: 0.2266  data: 0.0046  max mem: 4464\n",
            "2021/05/08 00:30:46\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:12:26\n",
            "2021/05/08 00:30:59\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:30:59\tINFO\t__main__\tValidation: accuracy = 0.9009701629141498\n",
            "2021/05/08 00:30:59\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/qnli/ce/qnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/qnli/ce/qnli-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/08 00:31:01\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [   0/3274]  eta: 0:10:01  lr: 1.3331297088169417e-05  sample/s: 22.907895796836875  loss: 0.1867 (0.1867)  time: 0.1838  data: 0.0092  max mem: 4464\n",
            "2021/05/08 00:32:55\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 500/3274]  eta: 0:10:34  lr: 1.2313174506210548e-05  sample/s: 21.30743166012182  loss: 0.2096 (0.2070)  time: 0.2324  data: 0.0047  max mem: 4464\n",
            "2021/05/08 00:34:49\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [1000/3274]  eta: 0:08:38  lr: 1.1295051924251682e-05  sample/s: 16.51751117184508  loss: 0.2120 (0.2068)  time: 0.2389  data: 0.0045  max mem: 4464\n",
            "2021/05/08 00:36:44\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [1500/3274]  eta: 0:06:46  lr: 1.0276929342292811e-05  sample/s: 14.951476279975866  loss: 0.1588 (0.2083)  time: 0.2332  data: 0.0045  max mem: 4464\n",
            "2021/05/08 00:38:38\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [2000/3274]  eta: 0:04:51  lr: 9.258806760333945e-06  sample/s: 18.895176312914597  loss: 0.1697 (0.2045)  time: 0.2150  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:40:32\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [2500/3274]  eta: 0:02:56  lr: 8.240684178375076e-06  sample/s: 18.832606694654604  loss: 0.1706 (0.2034)  time: 0.2201  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:42:26\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [3000/3274]  eta: 0:01:02  lr: 7.222561596416208e-06  sample/s: 22.63997279513767  loss: 0.1802 (0.2034)  time: 0.2152  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:43:28\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:12:27\n",
            "2021/05/08 00:43:41\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:43:41\tINFO\t__main__\tValidation: accuracy = 0.9099395936298736\n",
            "2021/05/08 00:43:41\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/qnli/ce/qnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/qnli/ce/qnli-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/08 00:43:43\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [   0/3274]  eta: 0:12:06  lr: 6.6646304215027494e-06  sample/s: 18.93866714228144  loss: 0.0813 (0.0813)  time: 0.2219  data: 0.0107  max mem: 4464\n",
            "2021/05/08 00:45:36\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 500/3274]  eta: 0:10:27  lr: 5.646507839543881e-06  sample/s: 14.913425554723931  loss: 0.1189 (0.1216)  time: 0.2329  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:47:30\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [1000/3274]  eta: 0:08:36  lr: 4.628385257585013e-06  sample/s: 17.331906330191448  loss: 0.1803 (0.1239)  time: 0.2329  data: 0.0044  max mem: 4464\n",
            "2021/05/08 00:49:24\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [1500/3274]  eta: 0:06:44  lr: 3.6102626756261456e-06  sample/s: 18.83419231352122  loss: 0.1325 (0.1207)  time: 0.2329  data: 0.0047  max mem: 4464\n",
            "2021/05/08 00:51:18\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [2000/3274]  eta: 0:04:50  lr: 2.5921400936672775e-06  sample/s: 18.652127896117758  loss: 0.1007 (0.1203)  time: 0.2225  data: 0.0046  max mem: 4464\n",
            "2021/05/08 00:53:12\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [2500/3274]  eta: 0:02:56  lr: 1.5740175117084096e-06  sample/s: 14.93529132656831  loss: 0.1144 (0.1196)  time: 0.2438  data: 0.0050  max mem: 4464\n",
            "2021/05/08 00:55:07\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [3000/3274]  eta: 0:01:02  lr: 5.558949297495419e-07  sample/s: 19.484533472387877  loss: 0.0693 (0.1196)  time: 0.2327  data: 0.0046  max mem: 4464\n",
            "2021/05/08 00:56:09\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:12:26\n",
            "2021/05/08 00:56:23\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:56:23\tINFO\t__main__\tValidation: accuracy = 0.9055464030752334\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/qnli/ce/qnli-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/qnli/ce/qnli-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/08 00:56:27\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/08 00:56:41\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:56:41\tINFO\t__main__\tTest: accuracy = 0.9099395936298736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99U7uAX2HI5"
      },
      "source": [
        "### 4.8 RTE task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-iYN-RSqEwF",
        "outputId": "ed8fba4f-daf7-4294-b18a-f9b11a8ba2fe"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/rte/ce/bert_base_uncased.yaml \\\n",
        "  --task rte \\\n",
        "  --log log/glue/rte/ce/bert_base_uncased.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 00:56:45.253820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/08 00:56:46\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/rte/ce/bert_base_uncased.yaml', log='log/glue/rte/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='rte', test_only=False, world_size=1)\n",
            "2021/05/08 00:56:46\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 697k/697k [00:00<00:00, 5.70MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00,  7.28ba/s]\n",
            "100% 1/1 [00:00<00:00, 21.34ba/s]\n",
            "100% 3/3 [00:00<00:00,  5.05ba/s]\n",
            "2021/05/08 00:56:54\tINFO\t__main__\tStart training\n",
            "2021/05/08 00:56:54\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/08 00:56:54\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/08 00:56:54\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/08 00:56:58\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 0/78]  eta: 0:00:24  lr: 1.9914529914529916e-05  sample/s: 12.960092821404542  loss: 0.7337 (0.7337)  time: 0.3153  data: 0.0067  max mem: 3175\n",
            "2021/05/08 00:57:11\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [50/78]  eta: 0:00:07  lr: 1.5641025641025644e-05  sample/s: 14.87858901320317  loss: 0.6906 (0.6904)  time: 0.2776  data: 0.0053  max mem: 4462\n",
            "2021/05/08 00:57:19\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:21\n",
            "2021/05/08 00:57:20\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow\n",
            "2021/05/08 00:57:20\tINFO\t__main__\tValidation: accuracy = 0.5451263537906137\n",
            "2021/05/08 00:57:20\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/rte/ce/rte-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/rte/ce/rte-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/08 00:57:21\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 0/78]  eta: 0:00:21  lr: 1.3247863247863248e-05  sample/s: 14.847435427717805  loss: 0.6926 (0.6926)  time: 0.2767  data: 0.0073  max mem: 4462\n",
            "2021/05/08 00:57:35\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [50/78]  eta: 0:00:07  lr: 8.974358974358976e-06  sample/s: 14.386879184185185  loss: 0.6757 (0.6781)  time: 0.2830  data: 0.0052  max mem: 4462\n",
            "2021/05/08 00:57:43\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:21\n",
            "2021/05/08 00:57:44\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow\n",
            "2021/05/08 00:57:44\tINFO\t__main__\tValidation: accuracy = 0.5740072202166066\n",
            "2021/05/08 00:57:44\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/rte/ce/rte-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/rte/ce/rte-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/08 00:57:45\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 0/78]  eta: 0:00:21  lr: 6.581196581196582e-06  sample/s: 14.9450300420009  loss: 0.6452 (0.6452)  time: 0.2752  data: 0.0076  max mem: 4462\n",
            "2021/05/08 00:57:59\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [50/78]  eta: 0:00:07  lr: 2.307692307692308e-06  sample/s: 14.721337108751523  loss: 0.6609 (0.6651)  time: 0.2765  data: 0.0053  max mem: 4462\n",
            "2021/05/08 00:58:06\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:21\n",
            "2021/05/08 00:58:07\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:07\tINFO\t__main__\tValidation: accuracy = 0.5595667870036101\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/rte/ce/rte-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/rte/ce/rte-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/08 00:58:12\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/08 00:58:13\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/rte/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:13\tINFO\t__main__\tTest: accuracy = 0.5740072202166066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjMUFiy2LFP"
      },
      "source": [
        "### 4.9 WNLI task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pHlvCY0qIVE",
        "outputId": "ad53b5df-ac3d-4815-f7bf-39335e4ebf34"
      },
      "source": [
        "!accelerate launch torchdistill/examples/hf_transformers/text_classification.py \\\n",
        "  --config torchdistill/configs/sample/glue/wnli/ce/bert_base_uncased.yaml \\\n",
        "  --task wnli \\\n",
        "  --log log/glue/wnli/ce/bert_base_uncased.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 00:58:17.249931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021/05/08 00:58:18\tINFO\t__main__\tNamespace(adjust_lr=False, config='torchdistill/configs/sample/glue/wnli/ce/bert_base_uncased.yaml', log='log/glue/wnli/ce/bert_base_uncased.txt', seed=None, student_only=False, task_name='wnli', test_only=False, world_size=1)\n",
            "2021/05/08 00:58:18\tINFO\t__main__\tDistributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "Use FP16 precision: True\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 100% 29.0k/29.0k [00:00<00:00, 1.03MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 16.63ba/s]\n",
            "100% 1/1 [00:00<00:00, 132.08ba/s]\n",
            "100% 1/1 [00:00<00:00, 48.47ba/s]\n",
            "2021/05/08 00:58:25\tINFO\t__main__\tStart training\n",
            "2021/05/08 00:58:25\tINFO\ttorchdistill.models.util\t[student model]\n",
            "2021/05/08 00:58:25\tINFO\ttorchdistill.models.util\tUsing the original student model\n",
            "2021/05/08 00:58:25\tINFO\ttorchdistill.core.training\tLoss = 1.0 * OrgLoss\n",
            "2021/05/08 00:58:28\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [ 0/20]  eta: 0:00:04  lr: 1.98e-05  sample/s: 16.484533114551144  loss: 0.9003 (0.9003)  time: 0.2478  data: 0.0051  max mem: 2057\n",
            "2021/05/08 00:58:32\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:04\n",
            "2021/05/08 00:58:32\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:32\tINFO\t__main__\tValidation: accuracy = 0.5633802816901409\n",
            "2021/05/08 00:58:32\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/wnli/ce/wnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/wnli/ce/wnli-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/08 00:58:34\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [ 0/20]  eta: 0:00:03  lr: 1.58e-05  sample/s: 22.494541017785433  loss: 0.8959 (0.8959)  time: 0.1820  data: 0.0041  max mem: 4061\n",
            "2021/05/08 00:58:37\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:03\n",
            "2021/05/08 00:58:37\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:37\tINFO\t__main__\tValidation: accuracy = 0.5774647887323944\n",
            "2021/05/08 00:58:37\tINFO\t__main__\tUpdating ckpt\n",
            "Configuration saved in ./resource/ckpt/glue/wnli/ce/wnli-bert-base-uncased/config.json\n",
            "Model weights saved in ./resource/ckpt/glue/wnli/ce/wnli-bert-base-uncased/pytorch_model.bin\n",
            "2021/05/08 00:58:39\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [ 0/20]  eta: 0:00:04  lr: 1.18e-05  sample/s: 20.114009490400502  loss: 0.6860 (0.6860)  time: 0.2037  data: 0.0048  max mem: 4061\n",
            "2021/05/08 00:58:43\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:03\n",
            "2021/05/08 00:58:43\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:43\tINFO\t__main__\tValidation: accuracy = 0.5633802816901409\n",
            "2021/05/08 00:58:43\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [ 0/20]  eta: 0:00:04  lr: 7.800000000000002e-06  sample/s: 19.686115113653965  loss: 0.7166 (0.7166)  time: 0.2071  data: 0.0039  max mem: 4061\n",
            "2021/05/08 00:58:47\tINFO\ttorchdistill.misc.log\tEpoch: [3] Total time: 0:00:03\n",
            "2021/05/08 00:58:47\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:47\tINFO\t__main__\tValidation: accuracy = 0.5211267605633803\n",
            "2021/05/08 00:58:47\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [ 0/20]  eta: 0:00:04  lr: 3.8000000000000005e-06  sample/s: 19.379091595840773  loss: 0.6719 (0.6719)  time: 0.2105  data: 0.0040  max mem: 4061\n",
            "2021/05/08 00:58:51\tINFO\ttorchdistill.misc.log\tEpoch: [4] Total time: 0:00:03\n",
            "2021/05/08 00:58:51\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:51\tINFO\t__main__\tValidation: accuracy = 0.5211267605633803\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading weights file ./resource/ckpt/glue/wnli/ce/wnli-bert-base-uncased/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./resource/ckpt/glue/wnli/ce/wnli-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "2021/05/08 00:58:56\tINFO\t__main__\t[Student: bert-base-uncased]\n",
            "2021/05/08 00:58:56\tINFO\t/usr/local/lib/python3.7/dist-packages/datasets/metric.py\tRemoving /root/.cache/huggingface/metrics/glue/wnli/default_experiment-1-0.arrow\n",
            "2021/05/08 00:58:56\tINFO\t__main__\tTest: accuracy = 0.5774647887323944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxE8LY2E3Z78"
      },
      "source": [
        "## 5. More sample configurations, models, datasets...\n",
        "You can find more [sample configurations](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/sample/) in the [***torchdistill***](https://github.com/yoshitomo-matsubara/torchdistill) repository.  \n",
        "If you would like to use larger datasets e.g., **ImageNet** and **COCO** datasets and models in `torchvision` (or your own modules), refer to the [official configurations](https://github.com/yoshitomo-matsubara/torchdistill/tree/master/configs/official) used in some published papers.  \n",
        "Experiments with such large datasets and models will require you to use your own machine due to limited disk space and session time (12 hours for free version and 24 hours for Colab Pro) on Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BEXt2243OE9"
      },
      "source": [
        "# Colab examples for knowledge distillation\n",
        "You can find Colab examples for knowledge distillation experiments in the [***torchdistill***](https://github.com/yoshitomo-matsubara/torchdistill) repository."
      ]
    }
  ]
}